{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gzip\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 39209/39209 [02:25<00:00, 269.72it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'testing/'\n",
    "\n",
    "df_train = pd.read_csv(data_path + 'Train.csv')\n",
    "\n",
    "def load_and_resize_image(filepath, shape):\n",
    "    \"\"\"\n",
    "    Loads and resizes an image from a filepath.\n",
    "    \"\"\"\n",
    "    raw_img = tf.io.read_file(filepath)\n",
    "    img_tensor = tf.io.decode_png(raw_img, channels=3)\n",
    "    img_tensor = tf.image.resize(img_tensor, tf.constant(shape[:2]))\n",
    "    return tf.cast(img_tensor, tf.float32) / 255.\n",
    "\n",
    "# load data\n",
    "imgs = np.zeros((len(df_train), 50, 50, 3))\n",
    "for i in tqdm(range(len(df_train)), colour='green'):\n",
    "    imgs[i] = load_and_resize_image(data_path + df_train['Path'].iloc[i], (50, 50, 3))\n",
    "\n",
    "labels = df_train['ClassId'].to_numpy()\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(imgs, labels, test_size=0.2)\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "test_labels = test_labels.reshape(-1, 1)\n",
    "\n",
    "# turn RGB to grayscale\n",
    "scaled_test_img = np.expand_dims(np.dot(test_data, [0.2990, 0.5870, 0.1140]), axis=3)\n",
    "scaled_train_img = np.expand_dims(np.dot(train_data,[0.2990,0.5870,0.1140]),axis=3)\n",
    "\n",
    "# normalizing\n",
    "def normalize(imgs):\n",
    "  mean = np.mean(imgs, axis=0)\n",
    "  std = np.std(imgs, axis=0)\n",
    "  imgs = (imgs-mean) / std\n",
    "  return imgs\n",
    "\n",
    "scaled_test_img = normalize(scaled_test_img)\n",
    "scaled_train_img = normalize(scaled_train_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                              patience=10,\n",
    "                              min_delta=0.001,\n",
    "                              verbose=1)\n",
    "\n",
    "checkpoint_CNN = ModelCheckpoint(filepath='models/checkpoint',\n",
    "                                 save_freq='epoch',\n",
    "                                 monitor='val_loss',\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 verbose=1)\n",
    "def CNN():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(50, 50, 1)),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='SAME'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='SAME'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu',kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        Dropout(0.3),\n",
    "        Dense(43, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting training\n",
    "\n",
    "Uncomment the following to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN()\n",
    "# history = model.fit(scaled_train_img, \n",
    "#                     train_labels, \n",
    "#                     epochs=30,\n",
    "#                     validation_split=0.2,\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[earlystopping,checkpoint_CNN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 50, 50, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 50, 50, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 50, 50, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 50, 50, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 25, 25, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 25, 25, 32)        0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 20000)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                640032    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 43)                1419      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 651,275\n",
      "Trainable params: 651,147\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x14fc4742f40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.summary()\n",
    "model.load_weights('models/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07643140852451324\n",
      "Test accuracy: 0.9931139945983887\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(scaled_test_img, test_labels, verbose=0)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(filepath, name_csv):\n",
    "    \"\"\"\"\n",
    "        Returns preprocessed data.  \n",
    "        \n",
    "        Args:  \n",
    "            - filepath: str, path to the data folder\n",
    "            - name_csv: str, name of the csv file\n",
    "            \n",
    "        Returns:  \n",
    "            - test_acc: float, accuracy of the model on the test set\n",
    "        \"\"\"\n",
    "    def preprocessing(data_path, name_csv): \n",
    "        df_train = pd.read_csv(data_path + name_csv)\n",
    "\n",
    "        def load_and_resize_image(filepath, shape):\n",
    "            \"\"\"\n",
    "            Loads and resizes an image from a filepath.\n",
    "            \"\"\"\n",
    "            raw_img = tf.io.read_file(filepath)\n",
    "            img_tensor = tf.io.decode_png(raw_img, channels=3)\n",
    "            img_tensor = tf.image.resize(img_tensor, tf.constant(shape[:2]))\n",
    "            return tf.cast(img_tensor, tf.float32) / 255.\n",
    "\n",
    "        # load data\n",
    "        imgs = np.zeros((len(df_train), 50, 50, 3))\n",
    "        \n",
    "        for i in tqdm(range(len(df_train)), colour='green', ascii=\" >=\"):\n",
    "            imgs[i] = load_and_resize_image(data_path + df_train['Path'].iloc[i], (50, 50, 3))\n",
    "\n",
    "        labels = df_train['ClassId'].to_numpy()\n",
    "\n",
    "        test_labels = labels.reshape(-1, 1)\n",
    "        \n",
    "        # turn RGB to grayscale\n",
    "        scaled_test_img = np.expand_dims(np.dot(imgs, [0.2990, 0.5870, 0.1140]), axis=3)\n",
    "\n",
    "        # normalizing\n",
    "        def normalize(imgs):\n",
    "            mean = np.mean(imgs, axis=0)\n",
    "            std = np.std(imgs, axis=0)\n",
    "            imgs = (imgs-mean) / std\n",
    "            return imgs\n",
    "\n",
    "        scaled_test_img = normalize(scaled_test_img)\n",
    "        \n",
    "        return scaled_test_img, test_labels\n",
    "    def CNN():\n",
    "        model = Sequential([\n",
    "            tf.keras.Input(shape=(50, 50, 1)),\n",
    "            Conv2D(32, kernel_size=3, activation='relu', padding='SAME'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(32, kernel_size=3, activation='relu', padding='SAME'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(2),\n",
    "            Dropout(0.3),\n",
    "            Flatten(),\n",
    "            Dense(32, activation='relu',kernel_regularizer=regularizers.l2(1e-5)),\n",
    "            Dropout(0.3),\n",
    "            Dense(43, activation='softmax')])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        return model\n",
    "    scaled_test_img, test_labels = preprocessing(filepath, name_csv)\n",
    "    model = CNN()\n",
    "    model.load_weights('models/checkpoint')\n",
    "    test_loss, test_accuracy = model.evaluate(scaled_test_img, test_labels, verbose=0)\n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m==========\u001b[0m| 39209/39209 [02:33<00:00, 255.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08162304013967514\n",
      "Test accuracy: 0.9921446442604065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9921446442604065"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission('testing/', 'Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "214ae085943121352a1c46be52870768915bb1800a79b1ca0ca08a53dfbff110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
